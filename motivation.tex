%TODO: cite Titan page, 80/20 percent thing
According to the center of accelerated application readiness at \acs{ORNL}, 80\% of application porting efforts 
to OLCF Titan (a \acs{GPU}-based system) was spent understanding and restructuring the code and 20\% on 
adding a new on-node parallel programming model (e.g. CUDA, OpenMP/OpenACC, etc) \cite{titan}.
Worse yet is that much of this effort often does not translate well from one application to another, making it a 
tedious and costly affair.
We expect this aspect of porting to be equally if not even more burdensome for other and future systems.
A typical result of this is that many applications do not perform as well as expected or as well as they are able 
to on other systems, and therefore may not be making full use of new systems and architectures.
Ideally, what users want is to be able to swap library implementations in and out across systems without 
sacrificing performance considerations in order to port code to multiple platforms better and more efficiently.
This would allow us to relieve some of the strain placed on application developers in porting their code to 
future system architectures.
It also helps in avoiding the need to maintain multiple versions or code paths to support all target architectures, 
which is another problem that still plagues many applications.

Another issue many applications face is that even after initial porting efforts to use new software and hardware 
technologies, they may have difficulty with scaling to exploit the full extent of the system and may not use the 
resources efficiently and obtain the expected performance characteristics.
It is rarely easy to discover and understand sources of such inefficiencies due both to code and architectural 
complexity as well as scale.
As such, being able to quickly and easily investigate large code bases as well as identify inefficient code 
patterns is key to application developers being able to port more effectively.

Determining usage patterns of various hardware, libraries, tools, new compiler or vendor package features, 
etc, and where it is most important to focus efforts, is an issue of great significance in achieving performance 
portability, and there is a lack of tools with which to gauge these patterns.
While XALT helps bridge this gap, there is still far more it could do to address not just what is used, but how it 
is used and the resulting degree of impact on applications.
We expect the level of detail offered by this tool to be able to provide greater insight into such concerns so as 
to help guide porting efforts.

%TODO: what are other models/libraries/tools doing to address this?

%TODO: OpenMP memory features? (oscar?)

%One last thing that I wouldn't mind knowing about, if it's possible, is how the code handles openMP vs. MPI 
%use. My benchmarks of this calculation on Eos and Titan showed that using many MPI ranks per node, even 
%32 on Eos, gave better performance than any use of openMP at all! I thought this was interesting. It's possible 
%that the MPI implementation is really well done and the openMP is not. Or, it could just be something about the 
%calculation itself. I think this is actually a very interesting CS problem.

%Thanks!

%-A
