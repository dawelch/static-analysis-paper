According to the center of accelerated application readiness at ORNL, 80\% of application porting efforts to OLCF Titan 
%(cite Titan page) 
(a \acs{GPU}-based system) was spent understanding and restructuring the code and 20\% on adding a new on-node parallel programming model (e.g. CUDA, OpenMP/OpenACC, etc). 
%(Citation needed for the 20 percent thing)
Worse yet is that much of this effort often does not translate well from one application to another, making it a tedious and costly affair.
We expect this aspect of porting to be equally if not even more burdensome for other and future systems.
Ideally, what a user wants is to be able to use a portable programming model that will allow it to build the source code as it is, swap library implementations in and out across systems, and run the code with different runtime parameters (e.g. \# of threads, MPI ranks, etc) in order to port code to multiple platforms more efficiently. This would allow us to relieve some of the strain placed on application developers in porting their code to future system architectures.
It also helps in avoiding the need to maintain multiple versions or code paths to support all target architectures, which is another problem that still plagues many applications.

Another issue many applications face is that even after initial porting efforts to use new software and hardware technologies, they may have difficulty with scaling to exploit the full extent of the system and may not use the resources efficiently and obtain the expected performance characteristics.
It is rarely easy to discover and understand the sources of inefficiencies due both to code and target architecture complexity as well as the scale.
As such, being able to quickly and easily investigate large code bases as well as identify efficient / inefficient code patterns is key to application developers being able to port more effectively.

%TODO: what are other models/libraries/tools doing to address this?

%TODO: OpenMP memory features? (oscar?)

%For example, a CP2K application developer knows that scalapack, a linear algebra library, is linked in his code but doesn't understand how it is being used in its code and how it is affecting the performance of the code on a given system.
%Ideally, she is interested to know how Scalapack is affecting the performance of a particular solver or calculation and how the parallel communication is affecting the performance on a given system.  

%She noticed that on a particular system Eos (a Xeon Phi based system) her code outperforms a GPU-based system Titan but doesn't know if its related to the implementation of the library, or how it is used in the code.
%She wants to understand why this is not an easy task as she is observing a lot of  performance difference.
%She thinks it may be related to the way the library is being invoked in the multi-threaded region of the code or it may be related on the quality of implementation of the library on a given system that favors one architecture over the other. 

%Furthermore, she wants to communicate her scientific library requirements to the system to the system administrator to make sure future systems have the proper library support for her code. 
 


%One last thing that I wouldn't mind knowing about, if it's possible, is how the code handles openMP vs. MPI use. My benchmarks of this calculation on Eos and Titan showed that using many MPI ranks per node, even 32 on Eos, gave better performance than any use of openMP at all! I thought this was interesting. It's possible that the MPI implementation is really well done and the openMP is not. Or, it could just be something about the calculation itself. I think this is actually a very interesting CS problem.

%Thanks!

%-A
