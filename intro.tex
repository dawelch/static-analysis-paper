Current and future memory hierarchies are becoming increasingly complex and are constantly evolving.
%TODO: "compute in memory"?
%TODO: "How much of these memories are exposed via the programming model? Do we want that? or do we want to keep the programming models "intact" and just add implicit hints to control the placement of variables to different types of memories. If so, would a tool like this one help? This will definately provide less restructuring. 
Though we know that such new memory technologies like \ac{HBM} and \ac{NVM} require novel approaches to application design, what is less clear is how to address the new architectures featuring them to optimally make use of these technologies or how to efficiently implement such strategies portably.
Furthermore, considering the potentially costly process of porting applications we need to explore partially or fully automated approaches to optimise existing codes written using de facto programming models.

Instead of focusing on extending programming models to abstract the complexity of these new architectures, we focus on  minimally intrusive, low overhead methods (implemented via tools) for identifying memory intensive regions of code so that developers may make better decisions regarding what parts of the application to focus their efforts on.
%TODO I think this sentence needs to say analysis to place the data to different memories -- place data structure X to memory Y. You need to mention this in this sentence.
%TODO Can you put what compiler analysis: examples are loopnest analysis - polyhedral, data structure accesses (what data you are accessing in loops and how), variable information (types of variables such as arrays, # dimensions, etc), static analysis on the shapes of data structures and iteration spaces.
We have created a way of performing analysis of code by combining information exported from the compiler with supplementary information obtained from a performance sampling tool into an \acs{SQL} database that we may query offline to find performance critical allocations.
The goal is to determine with as great an accuracy as possible which data structures influence application performance the most so that the developer may make better decisions with respect to their data placement in memory.
As we have previously explored this problem in a manual fashion (see section~\ref{sec:prevwork})\cite{Doudali:2017:CTE:3132402.3132418}, this work will start by focusing on recreating the kind of results we found by hand with the analysis tools described in this paper.
We will then go further to analyse a larger application that would be far too cumbersome to explore using our previous techniques, to demonstrate the flexibility of the tools and what other kinds of information can be discovered.
Furthermore, this work is currently focused on the analysis of Fortran code for simplicity, though it can be applied in much the same way to C/C++.

The rest of this paper is organised as follows: Section~\ref{sec:background} provides some background on the tools/methods used along with related work, Section~\ref{sec:prevwork} briefly details the relevant parts of our past work that we are attempting to replicate with the first part of our analysis, Section~\ref{sec:analysis} describes in detail the actual analysis performed, Section~\ref{sec:results} shows the results of the analysis and what it can say about the code, Section~\ref{sec:conclusion} evaluates the analysis and its results with respect to its usefulness on a larger and more diverse scale, and Section~\ref{sec:future} outlines the next steps for improving this work further.
