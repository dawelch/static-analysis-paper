% 2 pages -- Aaron
%\subsection{Overview}
 % Describe the system - create an image
%\subsection{Static Analysis}
%\subsection{Dynamic Analysis}
%\subsection{Database Schema}
For the automated analysis, rather than exclusively developing our own tools, we relied as much as possible on existing production quality tools and environments.
%TODO -- you need to explain what information are you extracting from the compiler --- you need to mention that you are extracting the Abstract Syntax Tree, and what other analysis and that you are leverging from gfortran intermediate representation data structures, where you are making a bridge from the intermediate program representation to SQL.
Our method extracts data about code directly from a production compiler \ac{GCC}, initially with an exclusive focus on Fortran, and optionally uses a scalable performance analysis tool (Allinea MAP)
%AS: cite Allinea MAP
to obtain information on run-time characteristics.
The optional dynamic analysis is run separately by the user to collect periodic samples of where the application is spending time.
These two sources of information are inserted into tables in an \acs{SQL} database, which then allows for queries to be run using one or both sources of data to gain insight into the code's execution.
An illustration of the this flow can be seen in Figure~\ref{fig:design}.
These steps will be addressed separately in the following subsections.
While we include components of a dynamic analysis in this way, we primarily emphasise the use of static information, with the dynamic analysis mostly used to narrow the focus more specifically on regions of particular interest and supplement the functional data (e.g. of routines invocations, loop trip counts, etc) obtained from the compiler with additional non-functional data (e.g. hardware counter information) obtained from the performance tool.

\begin{figure}
\begin{center}
\includegraphics[width=0.4\textwidth]{images/design.png}
\end{center}
\caption{Tool Design}
\label{fig:design}
\end{figure}

The result of this analysis is two-fold in that both system administrators and support staff as well as users (with optional supplementary run-time data) are able to get answers to questions about applications relevant to their own needs and interests.
We will demonstrate a few examples of this investigative ability in Section~\ref{sec:casestudy}.

%The desired result of this analysis is to achieve similar results to what the manual tweaking of our previous work found so that it can more realistically and scalably be applied to larger and more complex applications.
%To this end, we will compare the results from our previous work to what we achieved with our new tools to determine how well they are able to match up.
%We will then perform a reverse validation by applying the same analysis to a different and larger application that was not part of our previous work, modify the memory placement for the reported hotspots, and see how well the actual results fall in line with the expected results.
%This will be described in more detail in Section~\ref{sec:results}.
\subsection{Static Analysis}
%TODO: particular spec/features of fortran?
For the static analysis, we relied on the \ac{GCC} for its support of Fortran.
%This seems to be not very interesting for the paper -- this is just mechanics (Loading a module)
Using its plugin \acs{API}, we created a module to be loaded and run after the compiler finishes parsing the code.
%TODO: poly
%Additionally, it also registers a pass to be run while it's generating its GIMPLE \ac{IR} in order to get additional analysis information from the lower stages of the compiler.
The plugin runs through all the relevant data structures in the \ac{AST} representing the code to queue the data, then proceeds to dump them all at once in bulk transactions.
For the database, we used PostgreSQL for some of its advanced query support.
\subsubsection{Dumping the Data}
We did not attempt to create our own schema to represent the code, as it would not immediately be necessary due to the tables not being intended as a user-facing interface and our current focus on Fortran not requiring a new language agnostic schema.
%TODO: no possessive acronyms... :'(
Instead, we simply dumped out \ac{GCC}'s internal data structures such that each data structure was a table, and each member of the data structure was a column.
%TODO: technically not all tables...mention enums?
Common to all tables are two columns for the pointer to the data structure in memory during compilation, and a build ID unique to each compilation unit.
%TODO: necessary to very briefly explain primary/foreign keys, etc, or are they sufficiently self-describing?
Together, these two fields form the table's composite primary key to uniquely identify each record in the database.
Furthermore, this also allows us to dump all data in the most straightforward of manners - primitive types get dumped as their corresponding \acs{SQL} data type, and pointers to other objects as the raw value of those pointers.

This part of the data can be obtained automatically and without the direct involvement of the user, using the same methods used by XALT \cite{7081224}.
That is, a wrapper can be made for \ac{GCC} to intercept compilation and make it use the necessary plugin, and dump the data either to files, a system log, or directly to the database.
In addition to the primary data about the code itself, XALT is able to insert a unique identifier into each linked application.
An additional relation between this link ID and the compilation unit build IDs used for each of the files being linked must be made so that when applications are run, they can be tied back to each of the particular compilations and associated data, which we also need here for connecting the dynamic data at a later stage.
\subsubsection{Querying the Database}
Due to the way we identify records and store object references, we are able to use any columns representing pointers as foreign keys with which to join connected tables in the database.
One of the most important things we need the database to do for us is associate each code line with file lines.
A lot of our focus on the analysis is dependent on lines of code, so our queries were constructed so as to be able to clearly relate what file and line numbers are involved in an operation.
%TODO: explain what we do here better or not?
%In particular, we go through each statement and look up which file and lines they span using the line map table, traverse the expression trees for those statements, filter for expressions containing references to the symbol table, then finally check the symbol table to determine which particular symbols are being referenced.
%In this way, we are then able construct queries to tally the total number of references for every symbol referenced across any subset of lines.

%TODO: more poly?
In addition, we added to the database
polyhedral analysis information by utilising \acsp{GCC} Graphite framework\cite{trifunovic:inria-00551516}.
At this time, our use of the polytopes provided by Graphite is to do some basic reasoning about memory access order for arrays - notably, to find cases where a particular access is likely to result in either a greater or lesser number of cache misses than a typical in order access pattern.

We classify accesses into three categories based on a focus on the relation to the domain of the innermost loop.
For loops with iterators \texttt{i}, \texttt{j}, \texttt{k}, in that order, we refer to these categories as follows: the \texttt{[k][j]} class, the \texttt{[j][k]} class, and the \texttt{[j][i]} class.
In all cases, what we are really trying to address is the use of the innermost loop's iterator, or the lack thereof, so this is intended to serve as a simplification in that \texttt{j} is the "same" as \texttt{k + i} or \texttt{k - 1}.
Since Fortran indexes arrays in a column-major order, this means that the first \texttt{[k][j]} class is intended to represent accesses that are presumed to be both in order in memory and reasonably close in proximity between iterations, if not contiguous.
Similarly, the second \texttt{[j][k]} class represents the case where for sufficiently large data objects a new cache block may need to be fetched from memory for each iteration of the innermost loop, resulting in degraded performance compared to the first class.
Finally, the final \texttt{[j][i]} class is intended to represent the cases where the memory location being accessed remains constant throughout the entire innermost loop, only changing as often as any relevant enclosing loop iterates and thus carrying the expectation of being even slightly less expensive than the first class.
%TODO: reference this cost model paper in some way?
With these classifications, we can then weigh each access observed for a sample more or less based on the access type to create a cost model for our analysis.%\cite{5608348}
\subsection{Dynamic Analysis}
%TODO: cite map?
For the dynamic analysis, we used the ARM MAP sampling based profiler.
First, in order to link the static and dynamic data later, it needs to record the application's link ID that was added in during compilation.
Then, given a sample frequency and number of samples, it periodically probes the application for a set of predetermined metrics, then gathers the samples together to dump them out to an \acs{XML} file.
What we are interested in here is simply where each thread is at in the code at the time of sampling, specifically the file and line number for each frame of the stack.
We process the \acs{XML} file to extract this information via \ac{XSLT} to produce a \acs{CSV} file with the fields we want, and then upload it to the database so that we can achieve a count of how many times any given line is observed being executed by a thread.
We use this simple metric to determine in what regions of code the application is spending most of its time, so that we may focus only upon those regions.

While the static information is dumped once, the dynamic information can be obtained any desired number of times.
This allows for variable parametres to be used to see their influence on the code, such as the number of threads/processes or the problem size.
Which profile(s) to use for each analysis report must be specified directly by the user upon requesting the report.
Then, the tool can construct a query that links the compile-time and run-time data based upon the application's link ID and MAP run ID(s) to provide the desired results.
%\subsection{Combined Analysis}
%The real value of the analysis comes when combining the previously described static and dynamic aspects into a more comprehensive evaluation of the code.
%Using the results from MAP, we focus only on the subset of lines we recorded samples from, and use the information from the static analysis to obtain the symbols and their reference totals across those lines.
%We treat all references as equal, with no regard as to whether they are read or write accesses or what level of the memory hierarchy their current/local value may be stored in at the time of access.
%After we have all the reference count totals, we further sum all the totals and determine what percent of that total each symbol constitutes.
%It is this percentage we use to determine the relative impact on memory performance in lieu of the benefit factor from our previous work as described in Section~\ref{sec:prevwork}.
%TODO: poly
