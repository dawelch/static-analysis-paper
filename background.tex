%TODO: themes - portability, scalability
%TODO: not sure if these are the best citations here, and/or need more static analysis references, perhaps for a couple other non-compiler forms of static analysis? language/design related?
%TODO: This looks starting from sequential code
%TODO: Challenge is calculating system dependences: control flow, data flow, loop dependences -- all with alias analysis and array section analysis.
%TODO: Its hard today as there are limitations for these types of analysis: inter-procedural information -- how to propagate information across procedures through arguments or globals, etc.
%TODO: This gets even harder for parallel analysis where there are memory syncrhonizations, SPMD executions (threads taking different branches, etc).
%TODO: Historically, compilers optimize better loops or apply loop transformations rather than data structure optimizations because
%TODO: they tend to be more localized. 
%TODO: Early successes has been using sequential analysis to parallelize via automatic vectorization but this only focus on inner loops
%TODO: This has demostrated the importance of good dependence analysis and also good cost-models to measure the profitability of SIMD.
%TODO: To improve this, we need to look at optimizing using runtime information. This includes feedback directed optimizations (online and offload), where runtime functional data of the program is calcuated (frequency, value or phased behavior) to detect
%TODO: Hotspots of the code. Little of these feedback optimizaitons has focus on memory accesses. 


Research on static analysis of code has a very long history and has both taken many forms and addressed innumerable problem domains\cite{Andrade:2012:SAW:2355585.2355593}\cite{1194988}.
Our focus on utilising the compiler for our static analysis means that we can benefit from the dearth of both static analysis as well as compiler research.
Compilers face many difficult challenges in transforming code while ensuring that the meaning of the code has not been changed.
%TODO: did I actually mention focusing on arrays/loops in the intro?  I don't think I did...
What this means for us is that some of the same methods used to validate transformations can be applied to analysing memory use, particularly for array accesses within loops, where we expect to find the most memory-intensive regions.
In particular, one key concept is that of dependences between statements, such that the order of statements required to preserve this meaning must remain the same.
An integral process used to determine these dependences is to determine how variables are use and data travels through them in some form of dataflow analysis\cite{Feautrier1991}.
This is also commonly used to discover more complicated patterns of memory access, for enabling optimisations such as loop tiling to make better use of cache performance.
%TODO: mention other things, such as direction/distance vectors?

Among the more advanced methods compilers use to analyse code is to employ the polyhedral model\cite{Cousot:1978:ADL:512760.512770}\cite{Bagnara:2009:APC:1628316.1628385}\cite{benabderrahmane.10.cc}.
The polyhedral method is ideally suited for representing and reasoning about loops, although is generally restricted to operating on affine loop nests.
A primary benefit of using polytopes over other methods is their natural ability to compactly and mathematically represent access patterns within loops regardless of the bounds of their domains.
The model constructs polytopes for the $n$-dimensional space reflected by the loop nest's domain where iterations are represented as lattice points.
As a polytope is composed of the solutions to a finite number of linear inequalities, much of the analysis becomes a series of math problems on matrices.
While the model is highly expressive and powerful, its applicability constraints and relative computational expense for some operations on it have traditionally limited its practical use in compilation\cite{DBLP:journals/entcs/Simon10a}.
Nonetheless, a lot of work has been done on it for more precisely determining dependences\cite{Vasilache:2006:VDA:1183401.1183448} as well as for more advanced optimisation techniques\cite{Nieuwenhuizen2014AutovectorizationUP}\cite{5260526}.
While not heavily employed, progress into polyhedral analysis has made its way into both the \ac{GCC}\cite{trifunovic:inria-00551516} and LLVM\cite{grosser.11.impact} compilers.
Here, the focus is on a subset of loop nests referred to as \acp{SCoP}\cite{TBas}, which are defined in \cite{benabderrahmane.10.cc} as the maximal set of consecutive statements where loop bounds and conditionals are affine functions of the surrounding loop iterators and parametres.
%TODO: reference openscop? openscop\cite{Bas11}
%TODO: talk more about gcc internals/analysis?  gdfa citation?

Feedback optimizations are a variety  of techniques that aim to improve
the execution behavior of a program based on information on its current or
previous runtime behavior\cite{Smith:2000:OCF:351403.351408}. Runtime
information, which may be specific to a given instance of a program's
execution, helps the compiler direct its efforts to frequently
executed regions of code and make better judgements on what set of
optimizations can  improve the code.  There is a large body of work, including
our own\cite{10.1007/978-3-540-68555-5_22}, that  focuses on offline optimizations. Systems such as GEM,
IMPACT, SUIF, OpenUH, DCPI, FX!32 Morph, GCC, Alpha Compaq Compilers, SGI compilers, and PROMISE
perform high-level and object-level optimizations. Typical optimizations include
feedback directed inlining, partial dead code elimination,
instruction scheduling, code reordering and loop optimizations.
Several sets of  runtime information based on training sets of input data may be used to characterize the typical runtime behavior of the application.
A disadvantage of offline feedback optimization is that it recompiles the application based on the training set. Some of these
systems rely on profiling in which the data is agregated across multiple runs making the optimization
less specialized.
%A plus point: can do a lot of performance gathering

However, feedback optimizations are not limited to post mortem
optimizations.
%, such is the case of applications that adapts at runtime via parameritations or multiversioning, or partially compiled applications need to generate or reoptimize code on the fly.
Online feedback optimizations are attractive because they are less time consuming for the user, and may lead to specialized versions of code for the specific execution context.
%Didn't understand this point: The work of user-initiated recompilation, data gathering, source code modification, is leveraged.
There has been extensive research on using the runtime environment to
dynamically guide the compilation and execution of programs. Much of this work
has been carried out in the context of virtual machine technologies, typically
those supporting the Java programming language and C.  In general, the runtime
environment receives a statically compiled program which may be in a native
representation, or, as with the Java virtual machine, it may be in an
intermediate format (e.g. Java bytecode) for portability and/or to retain
desired semantic information.  In the latter case, it must be translated to
the native code either using an interpreter or (more commonly) a Just in Time
(JIT) compiler.  In some implementations, some portions of the program are
statically predicted to rarely execute; these may remain uncompiled to save
time and space. A mechanism would then have to be in place to generate this
code at runtime if necessary.

Systems such as  Jalapeno\cite{Alpern:2000:JVM:1011388.1011400}, HotSpot, and JavaVMs rely on feedback
information during runtume to generate or reoptimize code on the fly.
Other systems focus on online code reoptimizations via software with the
help of hardware. Dynamo, Cursoe, IA32EL, PIN, reoptimize
object code and stOMP, ADAPT\cite{Voss:2001:HAP:568014.379583}, Tempo, DyC,'C all create specialized versions
of the code during runtime.

A common strategy for online feedback optimization is to selectively recompile
identified ``hot spots'' with more aggressive optimizations.  Because programs
generally spend the majority of their execution time in a relatively small
fraction of the code, the typical selection procedure is to determine and
target the available optimizations towards these ``hot spots'' in the running
program.  Developing techniques for selecting hot spots and performing the
optimizations in an efficient manner is an important research area.

%TODO: talk about vectorisation?  or...?

%polyhedral predictive modeling\cite{park.13.ijpp}
%trace-based vectorisation potential\cite{holewinski.12.pldi}
%machine learning for auto-vectorisation\cite{stock.12.taco}

%TODO: what level of detail to go into dynamic analysis?  I don't really have as much on that side and am at a bit of a loss for what should be discussed here

%array dataflow analysis in x10\cite{Yuki:2013:ADA:2517327.2442520}
%TODO: use this? probabilistic miss equations\cite{1183947}

%TODO: static memory access pattern analysis on a massively parallel gpu
%TODO: hierarchical memory systems/sparse linear systems\cite{Wang:1999:PEM:1080585.1080587}
